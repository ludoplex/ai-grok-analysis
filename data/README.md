# Data Format Specification

## Directory Structure

```
data/
├── README.md                          ← this file
├── conversation-history.md            ← sidebar title listing (source of truth)
├── conversations/                     ← raw markdown conversation dumps (one per chat)
│   ├── image-collage-style-replacement.md
│   ├── multi-core-parallel-line-editor.md
│   └── ...
├── parsed/                            ← structured JSON (generated by scripts)
│   ├── conversation_index.json        ← master index from sidebar titles
│   ├── categorized_index.json         ← index enriched with categories
│   ├── conv_<slug>.json               ← individual parsed conversations
│   └── ...
└── analysis/                          ← analysis outputs
    ├── baselines.json                 ← per-category feature baselines
    ├── categories.json                ← category assignments
    ├── token_summary.json             ← corpus token frequency summary
    ├── tfidf_corpus.json              ← TF-IDF scores across conversations
    ├── temporal_tokens.json           ← monthly vocabulary timeline
    ├── temporal_drift.json            ← model update change-point detection
    ├── anomalies.json                 ← z-score anomalies per conversation
    └── full_anomaly_report.json       ← comprehensive multi-method report
```

## Data Formats

### conversation-history.md (Input — Manual)

Sidebar title dump from grok.com. Markdown format with sections by time period.
Each entry: `- Title (Date)` optionally with `×N` for duplicates.

### Conversation Dump Format (Input — Manual)

Place raw conversation dumps as `.md` files in `data/conversations/`.
The parser handles multiple formats flexibly:

**Preferred format (header-separated):**
```markdown
# Conversation Title

## User
What is the time complexity of quicksort?

## Grok
Quicksort has an average time complexity of O(n log n)...

## User
Follow-up question...

## Grok
Response...
```

**Also accepted:**
```markdown
**User:**
Question text

**Grok:**
Response text
```

### conversation_index.json (Generated)

Master index of all conversations with metadata.

```json
{
  "generated_at": "2026-02-04T21:35:00",
  "total_conversations": 120,
  "date_range": {
    "earliest": "2025-07-19",
    "latest": "2026-02-04"
  },
  "by_section": {
    "December 2025": 35,
    "November 2025": 42,
    "..."
  },
  "with_full_text": 0,
  "conversations": [
    {
      "id": "a1b2c3d4e5f6",
      "title": "Multi-Core Parallel Line Editor",
      "slug": "multi-core-parallel-line-editor",
      "date_raw": "Jan 18",
      "date_iso": "2026-01-18",
      "section": "This Year (Jan 2026)",
      "section_year": 2026,
      "section_month": null,
      "category_hint": null,
      "has_full_text": false,
      "duplicate_count": 1
    }
  ]
}
```

### conv_\<slug\>.json (Generated — Per Conversation)

Parsed conversation with full text and metadata.

```json
{
  "id": "a1b2c3d4e5f6",
  "title": "Multi-Core Parallel Line Editor",
  "slug": "multi-core-parallel-line-editor",
  "source_file": "multi-core-parallel-line-editor.md",
  "turns": [
    {
      "role": "user",
      "content": "How do I implement a parallel line editor...",
      "index": 0
    },
    {
      "role": "grok",
      "content": "Here's an approach using work-stealing queues...",
      "index": 1
    }
  ],
  "metadata": {
    "total_turns": 8,
    "user_turns": 4,
    "grok_turns": 4,
    "total_words_user": 245,
    "total_words_grok": 1823,
    "avg_grok_response_length": 455.8
  }
}
```

### baselines.json (Generated)

Per-category statistical baselines for anomaly detection.

```json
{
  "technical": {
    "count": 85,
    "void_proportion": {
      "mean": 0.0031,
      "std": 0.0022,
      "median": 0.0025,
      "min": 0.0,
      "max": 0.012,
      "q1": 0.0015,
      "q3": 0.0042
    },
    "tech_density": { "..." },
    "ttr": { "..." },
    "..."
  },
  "creative": { "..." },
  "_corpus": { "..." }
}
```

### full_anomaly_report.json (Generated)

Comprehensive anomaly detection output.

```json
{
  "summary": {
    "total_conversations": 120,
    "anomalous_conversations": 15,
    "anomaly_rate": 0.125,
    "total_anomalies": 42
  },
  "baselines": { "..." },
  "per_conversation": [
    {
      "title": "Some Conversation",
      "id": "abc123",
      "category": "technical",
      "anomaly_count": 5,
      "anomalies": [
        {
          "feature": "void_proportion",
          "value": 0.045,
          "z_score": 3.21,
          "baseline_mean": 0.003,
          "baseline_std": 0.002,
          "category": "technical",
          "direction": "high",
          "severity": "extreme"
        }
      ]
    }
  ],
  "isolation_forest": [
    {
      "title": "Outlier Conversation",
      "anomaly_score": -0.234,
      "outlier_features": { "void_proportion": 4.2 }
    }
  ],
  "temporal_drift": {
    "monthly_timeline": { "..." },
    "change_points": [
      {
        "transition": "2025-11 → 2025-12",
        "changes": [
          {
            "feature": "ttr",
            "prev_mean": 0.42,
            "curr_mean": 0.38,
            "change_magnitude": 2.1,
            "direction": "decrease"
          }
        ],
        "possible_model_update": true
      }
    ]
  }
}
```

## Pipeline Execution Order

```
1. parse_conversations.py --index          → conversation_index.json
2. (manually add .md dumps to conversations/)
3. parse_conversations.py --batch          → conv_*.json
4. categorize.py --index                   → categorized_index.json
5. categorize.py --conversations           → categories.json
6. tokenize_conversations.py --corpus      → token_summary.json
7. tokenize_conversations.py --tfidf       → tfidf_corpus.json
8. tokenize_conversations.py --temporal    → temporal_tokens.json
9. anomaly_detect.py --build-baselines     → baselines.json
10. anomaly_detect.py --full-report        → full_anomaly_report.json
```

## Temporal Coverage

| Period | Conversations | Notes |
|--------|--------------|-------|
| Jul 2025 | 2 | Earliest (MASM64, homework) |
| Oct 2025 | ~15 | SONiC, DirectX, fighting games |
| Nov 2025 | ~42 | Peak activity — SOPs, security, gaming |
| Dec 2025 | ~35 | Branchless programming, image editing |
| Jan 2026 | ~20 | Ballistics, ethics, misc |
| Feb 2026 | 3 | Current month |

## Known Grok Model Versions (Approximate)

| Date | Version | Source |
|------|---------|--------|
| ~Oct 2025 | Grok-2 | Public announcement |
| ~Dec 2025 | Grok-2 mini / update | Inferred from behavior |
| ~Jan 2026 | Grok-3 (beta?) | Unconfirmed |

Temporal drift analysis in `anomaly_detect.py --drift` attempts to detect
model update boundaries from statistical discontinuities in response features.

## Adding New Conversation Data

1. Export/copy conversation from grok.com
2. Save as `data/conversations/<slug>.md` using the header format above
3. Run: `python scripts/parse_conversations.py --conversation data/conversations/<slug>.md`
4. Re-run analysis: `python scripts/anomaly_detect.py --full-report data/parsed/`

The pipeline is designed for incremental addition — add conversations one at a
time and re-run analysis. Baselines update automatically.
